"""
Smol-Quant Terminal Interface & Safety Pipeline.

This script serves as the main entry point for the terminal-based interaction
with the Smol-Quant financial agent. It implements a 'Human-in-the-loop' style
architecture (replaced here by an LLM Evaluator) to ensure compliance with
financial regulations before displaying answers to the user.

Key Features:
1.  **LLM-as-a-Judge:** A secondary LLM validates agent outputs against compliance rules.
2.  **Self-Correction Loop:** If validation fails, the agent is fed the feedback 
    and asked to retry without losing conversation context.
3.  **Stateful Agent:** Manages conversation history via the smolagents framework.
"""

import logging
import os
import sys
from dotenv import load_dotenv
from openai import OpenAI

# Import the agent factory function
from agent.agent_builder import build_agent 

# Load environment variables (API keys, configuration)
load_dotenv()

# Configure Logging
# INFO level is sufficient for tracking the conversation flow and tool usage
logging.basicConfig(level=logging.INFO)

# Initialize a separate OpenAI client for the Evaluator (Judge).
# This ensures the safety check is decoupled from the agent's internal logic.
judge_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def evaluator_check(agent_response: str, original_question: str) -> dict:
    """
    Implements the 'LLM-as-a-Judge' pattern to validate agent responses.

    This function uses a smaller, efficient model (GPT-4o-mini) to act as a 
    Compliance Officer. It checks for two specific criteria:
    1. Financial Advice violations (imperative commands).
    2. Data Quality issues (hallucinations or empty responses).

    Args:
        agent_response (str): The text output generated by the financial agent.
        original_question (str): The initial query provided by the user.

    Returns:
        dict: A dictionary containing:
            - 'passed' (bool): True if the response is compliant.
            - 'feedback' (str): Explanation of failure if applicable.
    """
    # System prompt defining the rules for the evaluator.
    # Note: Rules are 'relaxed' to allow analysis but forbid explicit directives.
    system_prompt = """
    You are a helpful Compliance Assistant checking a financial report.
    
    Review the AI's response based on these Relaxed Rules:
    
    1. ALLOW ANALYSIS: The AI IS ALLOWED to describe trends, growth, and positive/negative sentiment (e.g., "The stock is performing well", "Strong upside potential", "Investors are bullish"). This is NOT financial advice, it is analysis.
    2. NO DIRECT COMMANDS: The AI should only avoid direct imperatives like "Buy this stock now!", "Sell immediately!", or "Put all your money in X".
    3. DATA QUALITY: The answer must not be empty or obvious gibberish. Technical terms like 'np.float64' are acceptable.
    
    Output format strictly:
    PASSED
    (or)
    FAILED: <Reason>
    """
    
    try:
        # Execute the judgment call
        response = judge_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"User Question: {original_question}\n\nAI Response: {agent_response}"}
            ]
        )
        
        # Parse the evaluator's decision
        verdict = response.choices[0].message.content
        
        if "PASSED" in verdict:
            return {"passed": True, "feedback": ""}
        else:
            # Extract the reason for failure to feed back into the agent
            return {"passed": False, "feedback": verdict.replace("FAILED:", "").strip()}
            
    except Exception as e:
        # Fallback mechanism: If the judge fails, default to passing to prevent system lockup.
        return {"passed": True, "feedback": f"Judge Error (Passed by default): {e}"}

def run_safe_pipeline(agent, user_input: str) -> str:
    """
    Executes the agent's logic wrapped in a self-correction loop.

    This function handles the multi-step process:
    1. Agent generates an initial response.
    2. Evaluator checks the response.
    3. If rejected, the feedback is injected back into the agent's context 
       (without resetting memory), allowing it to iteratively improve the answer.

    Args:
        agent: The initialized CodeAgent instance.
        user_input (str): The user's query.

    Returns:
        str: The final, compliant text response or a fallback error message.
    """
    print(f"[Agent] Processing query...")
    
    # 1. Initial Run
    # reset=True ensures no interference from previous distinct queries (optional design choice)
    response = agent.run(user_input, reset=True)
    
    # Define maximum attempts to prevent infinite loops during correction
    max_retries = 3
    
    for attempt in range(max_retries):
        # Convert response to string to handle potential object returns
        response_str = str(response)
        
        # 2. Evaluation Step
        evaluation = evaluator_check(response_str, user_input)
        
        if evaluation["passed"]:
            return response # Exit loop and return successful response
        
        # 3. Handling Failure
        print(f"\n[Warning] Compliance Alert (Attempt {attempt+1}/{max_retries}): {evaluation['feedback']}")
        print("[System] Initiating self-correction sequence based on feedback...")
        
        # 4. Correction Run
        # We construct a prompt containing the specific reason for rejection.
        correction_prompt = (
            f"Your previous answer was rejected by the Compliance Officer. "
            f"Reason: {evaluation['feedback']}. "
            f"Please rewrite your answer, strictly following the rules (Data only, no advice)."
        )
        
        # IMPORTANT: reset=False preserves the history. The agent now 'knows' 
        # it made a mistake and sees the specific feedback in its context window.
        response = agent.run(correction_prompt, reset=False)

    # Fallback if the agent fails to correct itself after max_retries
    return "[Error] I apologize, but I cannot provide a compliant answer to this query at the moment."

def main():
    """
    Main application entry point.
    Initializes the agent and runs the interactive Read-Eval-Print Loop (REPL).
    """
    print("--------------------------------------------------")
    print("Smol-Quant Terminal Interface (English Mode)")
    print("--------------------------------------------------")
    print("Loading Agent and Tools... please wait.")
    
    try:
        # Build the agent architecture (RAG, EDA, ImageGen tools)
        # This is a one-time initialization cost.
        agent = build_agent()
        print("[System] Ready. (Type 'exit' or 'quit' to close)")
    except Exception as e:
        print(f"[Critical Error] Failed to initialize agent: {e}")
        return

    # Continuous interaction loop
    while True:
        try:
            print("\n" + "="*50)
            user_input = input("You: ").strip()

            # Exit conditions
            if user_input.lower() in ["exit", "quit", "q"]:
                print("[System] Session ended.")
                break

            if not user_input:
                continue
            
            # Execute the pipeline (Generation + Evaluation + Correction)
            final_response = run_safe_pipeline(agent, user_input)

            print(f"\n[Agent] Final Answer:\n{final_response}")

        except KeyboardInterrupt:
            # Handle CTRL+C gracefully
            print("\n[System] Aborted by user.")
            break
        except Exception as e:
            print(f"[Runtime Error] An unexpected error occurred: {e}")

if __name__ == "__main__":
    main()